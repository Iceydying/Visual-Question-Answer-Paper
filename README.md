### Visual-Question-Answer-Paper

This is a paper list for Visual-Question-Answerï¼Œwhich also contains some related research areas.

**Keywords:** *Visual Question Answer*

#### Paper List

- `Information Fusion 2024` From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities **[[PDF](https://www.sciencedirect.com/science/article/abs/pii/S1566253524000484)]**
- `arxiv 2024` PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging **[[PDF](https://arxiv.org/pdf/2401.02797)]**  **[[code](https://github.com/jinlHe/PeFoMed)]**
- `arxiv 2024` Convincing Rationales for Visual Question Answering Reasoning **[[PDF](https://arxiv.org/pdf/2401.02797)]**  **[[code](https://github.com/lik1996/CRVQA2024)]**
- `arxiv 2024` Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays **[[PDF](https://arxiv.org/pdf/2402.08966)]**
- `ICASSP 2024/arxiv 2024` Prompt-based Personalized Federated Learning for Medical Visual Question Answering **[[PDF](https://arxiv.org/pdf/2402.09677)]**
- `arxiv 2024` Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering **[[PDF](https://arxiv.org/pdf/2402.12728)]**
- `arxiv 2024` SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM **[[PDF](https://arxiv.org/pdf/2403.04735)]**
- `ICDAR 2024/arxiv 2024` Multi-Page Document Visual Question Answering using Self-Attention Scoring Mechanism **[[PDF](https://arxiv.org/pdf/2404.19024)]**  **[[code](https://github.com/leitro/SelfAttnScoring-MPDocVQA)]**
- `AAAI 2024` NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario **[[PDF](https://doi.org/10.1609/aaai.v38i5.28253)]**  **[[code]( https://github.com/qiantianwen/NuScenes-QA)]**
- `AAAI 2024` VIGC: Visual Instruction Generation and Correction **[[PDF](https://doi.org/10.1609/aaai.v38i6.28338)]**  **[[code](https://opendatalab.github.io/VIGC)]**
- `AAAI 2024` FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering **[[PDF](https://doi.org/10.1609/aaai.v38i17.29823)]**  **[[code](https://github.com/leezythu/FlexKBQA)]**
- `AAAI 2024` T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering **[[PDF](https://doi.org/10.1609/aaai.v38i17.29884)]**  **[[code](https://github.com/T-SciQ/T-SciQ)]**
- `AAAI 2024` Bidirectional Contrastive Split Learning for Visual Question Answering **[[PDF](https://doi.org/10.1609/aaai.v38i19.30158)]**
- `AAAI 2024` BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions **[[PDF](https://doi.org/10.1609/aaai.v38i3.27999)]**  **[[code](https://github.com/mlpc-ucsd/BLIVA)]**
- `Neurips 2023` 3D-Aware Visual Question Answering about Parts, Poses and Occlusions **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/b783c44ba9adbc30344473dc633b4869-Paper-Conference.pdf)]**  **[[code](https://github.com/XingruiWang/3D-Aware-VQA)]**
- `Neurips 2023` AVIS: Autonomous Visual Information Seeking with Large Language Model Agent **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/029df12a9363313c3e41047844ecad94-Paper-Conference.pdf)]**
- `Neurips 2023` ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/d0b67349dd16b83b2cf6167fb4e2be50-Paper-Datasets_and_Benchmarks.pdf)]**  **[[code]( https://github.com/Jwoo5/ecg-qa)]**
- `Neurips 2023` EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/0c007ebef1d11fd48da6ce4f54687db6-Paper-Datasets_and_Benchmarks.pdf)]**  **[[code](https://github.com/baeseongsu/ehrxqa)]**
- `Neurips 2023` Emergent Communication in Interactive Sketch Question Answering **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/746cf1bc2337700f7f0c35c7b02638cc-Paper-Conference.pdf)]**  **[[code](https://github.com/MediaBrain-SJTU/ECISQA)]**
- `Neurips 2023` Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/47393e8594c82ce8fd83adc672cf9872-Paper-Conference.pdf)]**  **[[code](https://github.com/LinWeizheDragon/Retrieval-Augmented-Visual-Question-Answering)]**
- `Neurips 2023` Foundation Model is Efficient Multimodal Multitask Model Selector **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/687b7b2bdcc2ced577c0a989b44e7078-Paper-Conference.pdf)]**  **[[code](https://github.com/OpenGVLab/Multitask-Model-Selector)]**
- `Neurips 2023` Language Is Not All You Need: Aligning Perception with Language Models **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/e425b75bac5742a008d643826428787c-Paper-Conference.pdf)]**  **[[code](https://github.com/microsoft/unilm)]**
- `Neurips 2023` Large Language Models are Visual Reasoning Coordinators **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdf)]**  **[[code](https://github.com/cliangyu/Cola)]**
- `Neurips 2023` LoRA: A Logical Reasoning Augmented Dataset for Visual Question Answering **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/617ff5271b2b41dfb217a3b0f1b3d1be-Paper-Datasets_and_Benchmarks.pdf)]**  **[[code](https://lora-vqa.github.io/)]**  **[[code](https://github.com/CarolineGao/LoRA-Dataset.git)]**
- `Neurips 2023` Self-Chained Image-Language Model for Video Localization and Question Answering **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/f22a9af8dbb348952b08bd58d4734b50-Paper-Conference.pdf)]**  **[[code](https://github.com/Yui010206/SeViLA)]**
- `Neurips 2023` What You See is What You Read? Improving Text-Image Alignment Evaluation **[[PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/056e8e9c8ca9929cb6cf198952bf1dbb-Paper-Conference.pdf)]**

